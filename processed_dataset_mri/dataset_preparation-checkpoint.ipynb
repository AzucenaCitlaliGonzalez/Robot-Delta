{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c7cf8a-5d03-4ab8-8912-f2441bfc5443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre del archivo: 1.mat\n",
      "Tamaño de la imagen: (512, 512)\n"
     ]
    }
   ],
   "source": [
    "#Este código es para conocer\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "input_dir = r'C:\\Users\\End User\\Universidad\\Robot_Delta\\Unet_red\\archive\\dataset\\data'\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.mat'):\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            # La estructura cambia un poco, y todo está transpuesto (por convención MATLAB)\n",
    "            img = np.array(f['cjdata']['image']).T\n",
    "            print(f\"Nombre del archivo: {filename}\")\n",
    "            print(f\"Tamaño de la imagen: {img.shape}\")\n",
    "        break  # solo el primero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6532d571-ace7-4014-ba22-d403d95dd64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo image_data: <class 'h5py._hl.dataset.Dataset'>\n",
      "image_data.shape: (512, 512)\n",
      "Tipo tumor_mask_data: <class 'h5py._hl.dataset.Dataset'>\n",
      "tumor_mask_data.shape: (512, 512)\n",
      "img tipo: <class 'numpy.ndarray'>, shape: (512, 512)\n",
      "mask tipo: <class 'numpy.ndarray'>, shape: (512, 512)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "input_dir = r'C:\\Users\\End User\\Universidad\\Robot_Delta\\Unet_red\\archive\\dataset\\data'\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.mat'):\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            cjdata = f['cjdata']\n",
    "            \n",
    "            # Extraer directamente el contenido sin indexar más\n",
    "            image_data = cjdata['image']\n",
    "            tumor_mask_data = cjdata['tumorMask']\n",
    "            \n",
    "            print(f\"Tipo image_data: {type(image_data)}\")\n",
    "            print(f\"image_data.shape: {image_data.shape}\")\n",
    "            print(f\"Tipo tumor_mask_data: {type(tumor_mask_data)}\")\n",
    "            print(f\"tumor_mask_data.shape: {tumor_mask_data.shape}\")\n",
    "\n",
    "            # Intenta leer el array completo de la imagen y la máscara\n",
    "            img = image_data[()]\n",
    "            mask = tumor_mask_data[()]\n",
    "            \n",
    "            print(f\"img tipo: {type(img)}, shape: {img.shape}\")\n",
    "            print(f\"mask tipo: {type(mask)}, shape: {mask.shape}\")\n",
    "            \n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e29e365e-c12f-47ba-b6a1-bb1274e49f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset convertido y guardado en:\n",
      " - Imágenes: C:\\Users\\End User\\Universidad\\Robot_Delta\\Unet_red\\processed_dataset\\images\n",
      " - Máscaras: C:\\Users\\End User\\Universidad\\Robot_Delta\\Unet_red\\processed_dataset\\masks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "input_dir = r'C:\\Users\\End User\\Universidad\\Robot_Delta\\Unet_red\\archive\\dataset\\data'\n",
    "output_root = r'C:\\Users\\End User\\Universidad\\Robot_Delta\\Unet_red\\processed_dataset'\n",
    "output_images_dir = os.path.join(output_root, 'images')\n",
    "output_masks_dir = os.path.join(output_root, 'masks')\n",
    "\n",
    "os.makedirs(output_images_dir, exist_ok=True)\n",
    "os.makedirs(output_masks_dir, exist_ok=True)\n",
    "\n",
    "resize_shape = (512, 512)  # Igual que el tamaño original, sin cambio\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.mat'):\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        with h5py.File(filepath, 'r') as f:\n",
    "            cjdata = f['cjdata']\n",
    "            \n",
    "            img = cjdata['image'][()]\n",
    "            mask = cjdata['tumorMask'][()]\n",
    "            \n",
    "            # Normalizar imagen a 0-255 uint8\n",
    "            img_norm = (img - img.min()) / (img.max() - img.min()) * 255\n",
    "            img_norm = img_norm.astype(np.uint8)\n",
    "            \n",
    "            # Máscara 0/1 → 0/255 uint8\n",
    "            mask_img = (mask * 255).astype(np.uint8)\n",
    "            \n",
    "            # Convertir a PIL Images y resize si quieres (aquí mismo tamaño original)\n",
    "            img_pil = Image.fromarray(img_norm).resize(resize_shape, Image.BILINEAR)\n",
    "            mask_pil = Image.fromarray(mask_img).resize(resize_shape, Image.NEAREST)\n",
    "            \n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "            img_pil.save(os.path.join(output_images_dir, f'{base_name}.png'))\n",
    "            mask_pil.save(os.path.join(output_masks_dir, f'{base_name}_mask.png'))\n",
    "\n",
    "print(\"✅ Dataset convertido y guardado en:\")\n",
    "print(f\" - Imágenes: {output_images_dir}\")\n",
    "print(f\" - Máscaras: {output_masks_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41e80327-1004-4d0c-a622-b8c8dc515aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluación de Modelos U-Net (Tabla 2) ---\n",
      "Fuente: [1-3]\n",
      "--------------------------------------------------\n",
      "\n",
      "**Modelo: Standard U-Net**\n",
      "Epoch  | Loss     | Accuracy   | Val Loss     | Val Accuracy   \n",
      "----------------------------------------------------------------------\n",
      "1      | 0.1213     | 0.9831       | 0.0642         | 0.9829           \n",
      "2      | 0.0648     | 0.9831       | 0.0626         | 0.9829           \n",
      "3      | 0.0630     | 0.9832       | 0.0622         | 0.9829           \n",
      "4      | 0.0599     | 0.9835       | 0.0563         | 0.9840           \n",
      "5      | 0.0580     | 0.9840       | 0.0617         | 0.9843           \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "**Modelo: Attention-based U-Net model**\n",
      "Epoch  | Loss     | Accuracy   | Val Loss     | Val Accuracy   \n",
      "----------------------------------------------------------------------\n",
      "1      | 0.0944     | 0.9821       | 0.0732         | 0.9849           \n",
      "2      | 0.0789     | 0.9845       | 0.0741         | 0.9849           \n",
      "3      | 0.0734     | 0.9845       | 0.0735         | 0.9849           \n",
      "4      | 0.0728     | 0.9845       | 0.0745         | 0.9849           \n",
      "5      | 0.0711     | 0.9845       | 0.0706         | 0.9849           \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "**Modelo: Self-attention-based U-Net model**\n",
      "Epoch  | Loss     | Accuracy   | Val Loss     | Val Accuracy   \n",
      "----------------------------------------------------------------------\n",
      "1      | 0.1388     | 0.9746       | 0.0659         | 0.9823           \n",
      "2      | 0.0634     | 0.9810       | 0.0610         | 0.9823           \n",
      "3      | 0.0571     | 0.9815       | 0.0482         | 0.9849           \n",
      "4      | 0.0503     | 0.9829       | 0.0745         | 0.9860           \n",
      "5      | 0.0619     | 0.9827       | 0.0470         | 0.9852           \n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def mostrar_resultados_tabla2():\n",
    "    \"\"\"\n",
    "    Muestra los resultados de evaluación de los modelos U-Net\n",
    "    (Standard, Attention-based, Self-attention-based) a lo largo de las épocas,\n",
    "    basados en la información de la Tabla 2 del artículo.\n",
    "    \"\"\"\n",
    "\n",
    "    # Datos extraídos de la Tabla 2 del documento [1-3]\n",
    "    resultados = {\n",
    "        \"Standard U-Net\": [\n",
    "            {\"Epoch\": 1, \"Loss\": 0.1213, \"Accuracy\": 0.9831, \"Validation Loss\": 0.0642, \"Validation Accuracy\": 0.9829},\n",
    "            {\"Epoch\": 2, \"Loss\": 0.0648, \"Accuracy\": 0.9831, \"Validation Loss\": 0.0626, \"Validation Accuracy\": 0.9829},\n",
    "            {\"Epoch\": 3, \"Loss\": 0.0630, \"Accuracy\": 0.9832, \"Validation Loss\": 0.0622, \"Validation Accuracy\": 0.9829},\n",
    "            {\"Epoch\": 4, \"Loss\": 0.0599, \"Accuracy\": 0.9835, \"Validation Loss\": 0.0563, \"Validation Accuracy\": 0.9840},\n",
    "            {\"Epoch\": 5, \"Loss\": 0.0580, \"Accuracy\": 0.9840, \"Validation Loss\": 0.0617, \"Validation Accuracy\": 0.9843},\n",
    "        ],\n",
    "        \"Attention-based U-Net model\": [\n",
    "            {\"Epoch\": 1, \"Loss\": 0.0944, \"Accuracy\": 0.9821, \"Validation Loss\": 0.0732, \"Validation Accuracy\": 0.9849},\n",
    "            {\"Epoch\": 2, \"Loss\": 0.0789, \"Accuracy\": 0.9845, \"Validation Loss\": 0.0741, \"Validation Accuracy\": 0.9849},\n",
    "            {\"Epoch\": 3, \"Loss\": 0.0734, \"Accuracy\": 0.9845, \"Validation Loss\": 0.0735, \"Validation Accuracy\": 0.9849},\n",
    "            {\"Epoch\": 4, \"Loss\": 0.0728, \"Accuracy\": 0.9845, \"Validation Loss\": 0.0745, \"Validation Accuracy\": 0.9849},\n",
    "            {\"Epoch\": 5, \"Loss\": 0.0711, \"Accuracy\": 0.9845, \"Validation Loss\": 0.0706, \"Validation Accuracy\": 0.9849},\n",
    "        ],\n",
    "        \"Self-attention-based U-Net model\": [\n",
    "            {\"Epoch\": 1, \"Loss\": 0.1388, \"Accuracy\": 0.9746, \"Validation Loss\": 0.0659, \"Validation Accuracy\": 0.9823},\n",
    "            {\"Epoch\": 2, \"Loss\": 0.0634, \"Accuracy\": 0.9810, \"Validation Loss\": 0.0610, \"Validation Accuracy\": 0.9823},\n",
    "            {\"Epoch\": 3, \"Loss\": 0.0571, \"Accuracy\": 0.9815, \"Validation Loss\": 0.0482, \"Validation Accuracy\": 0.9849},\n",
    "            {\"Epoch\": 4, \"Loss\": 0.0503, \"Accuracy\": 0.9829, \"Validation Loss\": 0.0745, \"Validation Accuracy\": 0.9860},\n",
    "            {\"Epoch\": 5, \"Loss\": 0.0619, \"Accuracy\": 0.9827, \"Validation Loss\": 0.0470, \"Validation Accuracy\": 0.9852},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    print(\"--- Evaluación de Modelos U-Net (Tabla 2) ---\")\n",
    "    print(\"Fuente: [1-3]\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for modelo, epocas_data in resultados.items():\n",
    "        print(f\"\\n**Modelo: {modelo}**\")\n",
    "        print(f\"{'Epoch':<6} | {'Loss':<8} | {'Accuracy':<10} | {'Val Loss':<12} | {'Val Accuracy':<15}\")\n",
    "        print(\"-\" * 70)\n",
    "        for data in epocas_data:\n",
    "            print(\n",
    "                f\"{data['Epoch']:<6} | \"\n",
    "                f\"{data['Loss']:.4f}{' ':<4} | \"  # Adjusted spacing\n",
    "                f\"{data['Accuracy']:.4f}{' ':<6} | \" # Adjusted spacing\n",
    "                f\"{data['Validation Loss']:.4f}{' ':<8} | \" # Adjusted spacing\n",
    "                f\"{data['Validation Accuracy']:.4f}{' ':<11}\" # Adjusted spacing\n",
    "            )\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "# Ejecutar la función para mostrar los resultados\n",
    "mostrar_resultados_tabla2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d4108c4-1dc7-4446-a214-c05e9d79c62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ División completada:\n",
      " - Train: 2451 imágenes\n",
      " - Val: 613 imágenes\n",
      "Datos guardados en: C:\\Users\\End User\\Universidad\\Robot_Delta\\Unet_red\\processed_dataset\\splitted\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Ajusta a tu ruta base\n",
    "base_dir = r'C:\\Users\\End User\\Universidad\\Robot_Delta\\Unet_red\\processed_dataset'\n",
    "images_dir = os.path.join(base_dir, 'images')\n",
    "masks_dir = os.path.join(base_dir, 'masks')\n",
    "\n",
    "output_dir = os.path.join(base_dir, 'splitted')\n",
    "train_img_dir = os.path.join(output_dir, 'train', 'images')\n",
    "train_mask_dir = os.path.join(output_dir, 'train', 'masks')\n",
    "val_img_dir = os.path.join(output_dir, 'val', 'images')\n",
    "val_mask_dir = os.path.join(output_dir, 'val', 'masks')\n",
    "\n",
    "# Crea carpetas destino\n",
    "for d in [train_img_dir, train_mask_dir, val_img_dir, val_mask_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Obtener nombres base (sin _mask)\n",
    "image_files = [f for f in os.listdir(images_dir) if f.endswith('.png')]\n",
    "\n",
    "# Mezclar con semilla para que siempre sea igual\n",
    "random.seed(42)\n",
    "random.shuffle(image_files)\n",
    "\n",
    "split_idx = int(0.8 * len(image_files))\n",
    "train_files = image_files[:split_idx]\n",
    "val_files = image_files[split_idx:]\n",
    "\n",
    "# Función para copiar\n",
    "def copy_files(files, img_dest, mask_dest):\n",
    "    for f in files:\n",
    "        base = os.path.splitext(f)[0]\n",
    "        mask_name = f'{base}_mask.png'\n",
    "        shutil.copy2(os.path.join(images_dir, f), os.path.join(img_dest, f))\n",
    "        shutil.copy2(os.path.join(masks_dir, mask_name), os.path.join(mask_dest, mask_name))\n",
    "\n",
    "copy_files(train_files, train_img_dir, train_mask_dir)\n",
    "copy_files(val_files, val_img_dir, val_mask_dir)\n",
    "\n",
    "print(f\"✅ División completada:\")\n",
    "print(f\" - Train: {len(train_files)} imágenes\")\n",
    "print(f\" - Val: {len(val_files)} imágenes\")\n",
    "print(f\"Datos guardados en: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7c52535-ab91-4bc5-b41c-176604e04f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No hacer caso, se ocupará tensorflow en vez de pytorch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "\n",
    "class BrainTumorSegmentationDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.image_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.png')])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        base = os.path.splitext(img_name)[0]\n",
    "        mask_name = f'{base}_mask.png'\n",
    "\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        mask_path = os.path.join(self.masks_dir, mask_name)\n",
    "\n",
    "        # Cargar y convertir a tensor (C,H,W)\n",
    "        image = Image.open(img_path).convert('L')  # escala de grises\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        image = torch.from_numpy(np.array(image) / 255.).float().unsqueeze(0)\n",
    "        mask = torch.from_numpy(np.array(mask) / 255.).float().unsqueeze(0)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6729b6e8-b5e5-435f-9d5f-d32b8d99689a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoader listo: 2451 train imgs, 613 val imgs\n"
     ]
    }
   ],
   "source": [
    "#No hacer caso, se ocupará tensorflow en vez de pytorch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Ajusta rutas según donde hayas guardado\n",
    "train_images = r'C:\\Users\\End User\\Universidad\\Robot_Delta\\Unet_red\\processed_dataset\\splitted\\train\\images'\n",
    "train_masks  = r'C:\\Users\\End User\\Universidad\\Robot_Delta\\Unet_red\\processed_dataset\\splitted\\train\\masks'\n",
    "val_images   = r'C:\\Users\\End User\\Universidad\\Robot_Delta\\Unet_red\\processed_dataset\\splitted\\val\\images'\n",
    "val_masks    = r'C:\\Users\\End User\\Universidad\\Robot_Delta\\Unet_red\\processed_dataset\\splitted\\val\\masks'\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset = BrainTumorSegmentationDataset(train_images, train_masks)\n",
    "val_dataset   = BrainTumorSegmentationDataset(val_images, val_masks)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"✅ DataLoader listo: {len(train_dataset)} train imgs, {len(val_dataset)} val imgs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d0dc7a-4db5-4dd1-bb18-dba8c1991b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
